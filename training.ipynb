{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy8V6l7mnT5x",
        "outputId": "a8130f6f-8bf7-48b9-ed73-df0a5e4ee5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: {'hola': 0, 'adiós': 1, 'estás?': 2, 'es': 3, '¿cómo': 4, 'nombre?': 5, '¿qué': 6, 'tu': 7}\n",
            "Epoch [50/500], Loss: 0.0049\n",
            "Epoch [100/500], Loss: 0.0016\n",
            "Epoch [150/500], Loss: 0.0008\n",
            "Epoch [200/500], Loss: 0.0005\n",
            "Epoch [250/500], Loss: 0.0003\n",
            "Epoch [300/500], Loss: 0.0002\n",
            "Epoch [350/500], Loss: 0.0002\n",
            "Epoch [400/500], Loss: 0.0001\n",
            "Epoch [450/500], Loss: 0.0001\n",
            "Epoch [500/500], Loss: 0.0001\n",
            "Modelo guardado como chatbot_model.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Datos de ejemplo para entrenar el chatbot (preguntas y respuestas)\n",
        "data = [\n",
        "    (\"hola\", \"¡Hola! ¿Cómo estás?\"),\n",
        "    (\"¿cómo estás?\", \"Estoy bien, gracias por preguntar.\"),\n",
        "    (\"adiós\", \"¡Adiós! Cuídate.\"),\n",
        "    (\"¿qué es tu nombre?\", \"Soy un chatbot simple.\"),\n",
        "]\n",
        "\n",
        "# Preprocesamiento del texto\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "def build_vocab(data):\n",
        "    words = set()\n",
        "    for question, answer in data:\n",
        "        words.update(tokenize(question))\n",
        "    return {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "vocab = build_vocab(data)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulario:\", vocab)\n",
        "\n",
        "# Convertir texto a tensores\n",
        "def text_to_tensor(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    indices = [vocab[word] for word in tokens if word in vocab]\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Crear una red neuronal simple\n",
        "class ChatbotModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
        "        super(ChatbotModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.fc(h_n[-1])\n",
        "        return out\n",
        "\n",
        "# Hiperparámetros\n",
        "embed_size = 8\n",
        "hidden_size = 16\n",
        "num_classes = len(data)\n",
        "model = ChatbotModel(vocab_size, embed_size, hidden_size, num_classes)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for idx, (question, answer) in enumerate(data):\n",
        "        x = text_to_tensor(question, vocab).unsqueeze(0)  # Agregar dimensión batch\n",
        "        y = torch.tensor([idx])  # Índice de la respuesta correcta\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_fn(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(data):.4f}\")\n",
        "\n",
        "# Guardar el modelo en formato TorchScript\n",
        "example_input = text_to_tensor(\"hola\", vocab).unsqueeze(0)\n",
        "scripted_model = torch.jit.trace(model, example_input)\n",
        "torch.jit.save(scripted_model, 'chatbot_model.pt')\n",
        "print(\"Modelo guardado como chatbot_model.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Sma0NAq5wG",
        "outputId": "d5b7bfb5-8423-48fd-9168-8eba04dec271"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(model,\n",
        "                  example_input,\n",
        "                  \"chatbot_model.onnx\",\n",
        "                  input_names=['input'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n"
      ],
      "metadata": {
        "id": "rpL5F4TFnfSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9151b50e-00f6-4590-c329-26bbfce76649"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Datos de ejemplo\n",
        "data = [\n",
        "    (\"hola\", \"¡Hola! ¿Cómo estás?\"),\n",
        "    (\"¿cómo estás?\", \"Estoy bien, gracias por preguntar.\"),\n",
        "    (\"adiós\", \"¡Adiós! Cuídate.\"),\n",
        "    (\"¿qué es tu nombre?\", \"Soy un chatbot simple.\"),\n",
        "]\n",
        "\n",
        "# Preprocesamiento del texto\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "def build_vocab(data):\n",
        "    words = set()\n",
        "    for question, answer in data:\n",
        "        words.update(tokenize(question))\n",
        "    return {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "vocab = build_vocab(data)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Convertir texto a tensores\n",
        "def text_to_tensor(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    indices = [vocab.get(word, 0) for word in tokens]\n",
        "    return torch.tensor(indices, dtype=torch.float32)  # Importante: float32\n",
        "\n",
        "# Red neuronal sin LSTM (para evitar Cast)\n",
        "class ChatbotModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
        "        super(ChatbotModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())  # Asegurar float32\n",
        "        x = x.mean(dim=1)  # Promedio de los embeddings\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Parámetros del modelo\n",
        "embed_size = 8\n",
        "hidden_size = 16\n",
        "num_classes = len(data)\n",
        "model = ChatbotModel(vocab_size, embed_size, hidden_size, num_classes)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for idx, (question, answer) in enumerate(data):\n",
        "        x = text_to_tensor(question, vocab).unsqueeze(0)\n",
        "        y = torch.tensor([idx])\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_fn(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(data):.4f}\")\n",
        "\n",
        "# Exportar modelo a ONNX (opset 11)\n",
        "example_input = text_to_tensor(\"hola\", vocab).unsqueeze(0)\n",
        "model = model.float()  # Asegurar float32\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    example_input,\n",
        "    \"chatbot_model.onnx\",\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size', 1: 'sequence_length'}, 'output': {0: 'batch_size'}},\n",
        "    opset_version=11  # Usar opset 11\n",
        ")\n",
        "\n",
        "print(\"Modelo exportado como chatbot_model.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVdwkdj8wdTU",
        "outputId": "e1076645-4924-4892-8a21-3ce4ccb456d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 0.0035\n",
            "Epoch [100/500], Loss: 0.0009\n",
            "Epoch [150/500], Loss: 0.0004\n",
            "Epoch [200/500], Loss: 0.0002\n",
            "Epoch [250/500], Loss: 0.0002\n",
            "Epoch [300/500], Loss: 0.0001\n",
            "Epoch [350/500], Loss: 0.0001\n",
            "Epoch [400/500], Loss: 0.0001\n",
            "Epoch [450/500], Loss: 0.0000\n",
            "Epoch [500/500], Loss: 0.0000\n",
            "Modelo exportado como chatbot_model.onnx\n"
          ]
        }
      ]
    }
  ]
}